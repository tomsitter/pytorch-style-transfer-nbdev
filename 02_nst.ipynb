{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp nst\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "\n",
    "from pytorch_nst.config import device, content_layers_default, style_layers_default\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    ''' Calculate mse loss between two feature maps from the same layer'''\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # must detach target from the tree so that\n",
    "        # gradient can be computed dynamically\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Calculate mse loss and return the input. \n",
    "        # Since we return the layer unaltered, this layer has no actual impact on subsequent layers'''\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    ''' A gram matrix will calculate the relation between the filter activated caused by an image.\n",
    "        Multiple filters being activated together by in image can be thought of as it's stlye\n",
    "    '''\n",
    "    batch_size, num_features, height, width = input.size()\n",
    "    features = input.view(batch_size*num_features, height*width)\n",
    "    G = torch.mm(features, features.t()) # gram product\n",
    "    # normalize by dividing by number of elements. \n",
    "    # This is needed so that layers with different dimensions have the same weight\n",
    "    return G.div(batch_size*num_features*height*width)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    ''' Calculate mse loss of the gram matrix of the two feature maps from the same layer'''\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    ''' The VGG19 network was pre-trained with normalized images, so we will apply the same normalization\n",
    "        to images fed into this application\n",
    "    '''\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        \n",
    "        self.mean = mean.clone().view(-1, 1, 1)\n",
    "        self.std = std.clone().view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        #normalize image\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_style_model_and_losses(cnn, \n",
    "                           normalization_mean, normalization_std,\n",
    "                           style_img, content_img, \n",
    "                           content_layers=content_layers_default,\n",
    "                           style_layers=style_layers_default):\n",
    "    '''\n",
    "    We're going to build a model where we insert our content/style layers into the pre-trained cnn\n",
    "    These layers are \"transparent\", as they pass along the input unaltered, but have their own custom\n",
    "    loss functions that our optimizer will use on the generated image.\n",
    "    '''\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization(normalization_mean, \n",
    "                                  normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i+=1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "        \n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # trim layers after the last content/style loss layer, since we don't need them\n",
    "    for i in range(len(model) -1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i+1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "def run_style_transfer(cnn, \n",
    "                       normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, \n",
    "                       num_steps=300, \n",
    "                       style_layers=None, style_weight=1000000, content_weight=1):\n",
    "\n",
    "    if not style_layers:\n",
    "        style_layers = style_layers_default\n",
    "\n",
    "    print(\"Building the style transfer model..\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "                normalization_mean, normalization_std, \n",
    "                style_img, content_img, \n",
    "                style_layers=style_layers)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print(\"Optimizing...\")\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # clip pixel values to between 0 and 1\n",
    "            input_img.data.clamp_(0,1)\n",
    "\n",
    "            # Run a step forward, calculate loss of our content/style layers\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score=0\n",
    "            content_score=0\n",
    "\n",
    "            for s1 in style_losses:\n",
    "                style_score += s1.loss\n",
    "            for c1 in content_losses:\n",
    "                content_score += c1.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f'run {run}')\n",
    "                print(f'Style Loss: {round(style_score.item(), 2)}  '\n",
    "                      f'Content Loss: {round(content_score.item(), 2)}')\n",
    "                print()\n",
    "            \n",
    "            return style_score + content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # clip pixel values to between 0 and 1\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-style-transfer-nbdev",
   "language": "python",
   "name": "pytorch-style-transfer-nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
