{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp nst\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'normalization_mean' from 'pytorch_nst.config' (/home/tom/projects/pytorch-style-transfer-nbdev/pytorch_nst/config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc4b3573c448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_nst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_layers_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_layers_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mContentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'normalization_mean' from 'pytorch_nst.config' (/home/tom/projects/pytorch-style-transfer-nbdev/pytorch_nst/config.py)"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "import pdb\n",
    "\n",
    "from pytorch_nst.config import device, content_layers_default, style_layers_default, normalization_mean, normalization_std\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    ''' Calculate mse loss between two feature maps from the same layer'''\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # must detach target from the tree so that\n",
    "        # gradient can be computed dynamically\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Calculate mse loss and return the input. \n",
    "        # Since we return the layer unaltered, this layer has no actual impact on subsequent layers'''\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    ''' A gram matrix will calculate the relation between the filter activated caused by an image.\n",
    "        Multiple filters being activated together by in image can be thought of as it's stlye\n",
    "    '''\n",
    "    batch_size, num_features, height, width = input.size()\n",
    "    features = input.view(batch_size*num_features, height*width)\n",
    "    G = torch.mm(features, features.t()) # gram product\n",
    "    # normalize by dividing by number of elements. \n",
    "    # This is needed so that layers with different dimensions have the same weight\n",
    "    return G.div(batch_size*num_features*height*width)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    ''' Calculate mse loss of the gram matrix of the two feature maps from the same layer'''\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    ''' The VGG19 network was pre-trained with normalized images, so we will apply the same normalization\n",
    "        to images fed into this application\n",
    "    '''\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        \n",
    "        self.mean = mean.clone().view(-1, 1, 1)\n",
    "        self.std = std.clone().view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        #normalize image\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_style_model_and_losses(cnn, \n",
    "                               style_img, \n",
    "                               content_img, \n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    '''\n",
    "    We're going to build a model where we insert our content/style layers into the pre-trained cnn\n",
    "    These layers are \"transparent\", as they pass along the input unaltered, but have their own custom\n",
    "    loss functions that our optimizer will use on the generated image.\n",
    "    '''\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization(normalization_mean, \n",
    "                                  normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    pool, conv, bn = 1, 1, 1\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = f'conv{pool}_{conv}'\n",
    "            conv+=1\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu{pool}_{conv}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool{pool}'\n",
    "            conv = 1\n",
    "            pool += 1\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn{bn}'\n",
    "            bn += 1\n",
    "        else:\n",
    "            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "        #print(name)\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{pool}_{conv}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "        \n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{pool}_{conv}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # trim layers after the last content/style loss layer, since we don't need them\n",
    "    for i in range(len(model) -1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i+1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "def run_style_transfer(cnn, \n",
    "                       content_img, \n",
    "                       style_img, \n",
    "                       input_img, \n",
    "                       num_steps=300, \n",
    "                       style_layers=None, style_weight=1000000, content_weight=1):\n",
    "\n",
    "    if not style_layers:\n",
    "        style_layers = style_layers_default\n",
    "\n",
    "    print(\"Building the style transfer model..\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "                                                                     style_img, \n",
    "                                                                     content_img, \n",
    "                                                                     style_layers=style_layers)\n",
    "    \n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print(\"Optimizing...\")\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # clip pixel values to between 0 and 1\n",
    "            input_img.data.clamp_(0,1)\n",
    "\n",
    "            # Run a step forward, calculate loss of our content/style layers\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score=0.\n",
    "            content_score=0.\n",
    "\n",
    "            for s1 in style_losses:\n",
    "                style_score += s1.loss\n",
    "            for c1 in content_losses:\n",
    "                content_score += c1.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f'run {run}')\n",
    "                # print(f'Style Loss: {round(style_score.item(), 2)}  '\n",
    "                #      f'Content Loss: {round(content_score.item(), 2)}')\n",
    "                print(f'Style Loss: {style_score}  '\n",
    "                      f'Content Loss: {content_score}')\n",
    "                print()\n",
    "                \n",
    "            \n",
    "            return style_score + content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # clip pixel values to between 0 and 1\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-style-transfer-nbdev",
   "language": "python",
   "name": "pytorch-style-transfer-nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
